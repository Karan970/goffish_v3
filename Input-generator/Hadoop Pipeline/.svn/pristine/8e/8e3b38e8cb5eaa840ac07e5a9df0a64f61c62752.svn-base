package in.dream_lab.hadoopPipeline.cc;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
/*
 * Job ID : 5
 * Job Name : 
 * Job Description: Append Subgraph & Partition ID to remote Adjacency  	
 *

*/

//emit  from SPLALFileMapper  V_id 0!P_id#SG_id $ <E_loc, V_loc>,+ 
//emit from SPRSALFileMapper  V_id	1!V_rem : E_rem : P_rem : SG_rem
public class SPRLALReducer extends Reducer< LongWritable, Text,Text,Text>  {

	@Override
	protected void reduce(LongWritable key, Iterable<Text> values,
			Context context)
			throws IOException, InterruptedException {
		// TODO parse the input based on flag
		//StringBuilder reduceKey = new StringBuilder();
		//StringBuilder reduceValue = new StringBuilder();
		StringBuilder localEdges = new StringBuilder();
		StringBuilder remoteEdges = new StringBuilder();
		//int PartitionID, subgraphID;
		String PartitionSubgraphID="";
		// input SPRSALFileMapper V 1*V_rem : E_rem : P_rem : SG_rem
		for(Text v: values){
			String mapValue=v.toString();
			String[] SplitMapValue = mapValue.split("!");
			if(SplitMapValue[0].equals("1")){//RemoteEdge
				
				//emit from SPRSALFileMapper  V_id	1!V_rem : E_rem : P_rem : SG_rem			
				
				//System.out.println("TESTR1: "+mapValue);
				
				remoteEdges.append("1:").append(SplitMapValue[1]).append(",");
			}else{//Local edges
				
				//emit  from SPLALFileMapper  V_id 0!P_id#SG_id $ <E_loc, V_loc>,+
				
				//System.out.println("TESTR2: "+mapValue);
				
				//P_id#SG_id$ <E_loc, V_loc>+],.....
				String[] LocalEdgeRecord=SplitMapValue[1].split("@",-1);
				PartitionSubgraphID=LocalEdgeRecord[0];
				if(!LocalEdgeRecord[1].isEmpty()){
				for(String edge: LocalEdgeRecord[1].split(",")){
					if(!edge.isEmpty()){
					localEdges.append("0:").append(edge).append(","); }
					
				}
			}
			}
		}
		
		
		context.write(new Text(PartitionSubgraphID), new Text(key.toString()+"#"+localEdges.toString()+"#"+remoteEdges.toString()));
		
	}

}
